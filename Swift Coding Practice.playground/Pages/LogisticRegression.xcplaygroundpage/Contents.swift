/*:
[Previous](@previous)
****
# é‚è¼¯è¿´æ­¸

é‚è¼¯å›æ­¸ï¼Œå³é‚è¼¯æ¨¡å‹ï¼Œä¹Ÿè­¯ä½œã€Œè©•å®šæ¨¡å‹ã€ã€ã€Œåˆ†é¡è©•å®šæ¨¡å‹ã€æ˜¯é›¢æ•£é¸æ“‡æ³•æ¨¡å‹ä¹‹ä¸€ï¼Œå±¬æ–¼å¤šé‡è®Šé‡åˆ†æç¯„ç–‡ï¼Œæ˜¯ç¤¾æœƒå­¸ã€ç”Ÿç‰©çµ±è¨ˆå­¸ã€è‡¨åºŠã€æ•¸é‡å¿ƒç†å­¸ã€è¨ˆé‡ç¶“æ¿Ÿå­¸ã€å¸‚å ´è¡ŒéŠ·ç­‰çµ±è¨ˆå¯¦è­‰åˆ†æçš„å¸¸ç”¨æ–¹æ³•ã€‚

![](Linear_regression.png)

## é‚è¼¯åˆ†å¸ƒå…¬å¼

![](1.png)

å…¶ä¸­åƒæ•¸ğ›ƒå¸¸ç”¨æœ€å¤§ä¼¼ç„¶ä¼°è¨ˆã€‚

*/
import Foundation

public class LogisticRegression<T: SummableMultipliable> {
	public var x: [[T]]
	public var y: [[T]]
	public var W: [[T]]
	public var b: [T]
	public var settings: [String: Int] = [:]
	
	public init(settings: [String:Any]) {
		self.x = settings["input"] as! [[T]]
		self.y = settings["label"] as! [[T]]
		self.W = Math.zeroMat(settings["n_in"] as! Int,col: settings["n_out"] as! Int)
		self.b = Math.zeroVec(settings["n_out"] as! Int)
		self.settings = ["log level" : 1] // 0 : nothing, 1 : info, 2: warn
	}
	public func train(settings: [String: Any]) {
		var lr:T = T() is Double ? 0.1 as! T : 0.1.toInt() as! T
		var epochs:T = T() is Double ? 200.0 as! T : 200 as! T // default

		if let i = settings["input"] {
			self.x = i as! [[T]]
		}
		if let lrValue = settings["lr"] {
			lr = lrValue as! T
		}
		if let eValue = settings["epochs"] {
			epochs = eValue as! T
		}
		var currentProgress:T = Math.one()
		for i in 0..<epochs.toInt() {
			let probYgivenX = Math.softmaxMat(Math.addMatVec(Math.mulMat(self.x,mat2: self.W),vec: self.b))
			let deltaY = Math.minusMat(self.y,mat2: probYgivenX)
			
			let deltaW = Math.mulMat(Math.transpose(self.x),mat2: deltaY)
			let deltaB = Math.meanMatAxis(deltaY,axis: 0)
			
			self.W = Math.addMat(self.W,mat2: Math.mulMatScalar(deltaW,scalar: lr))
			self.b = Math.addVec(self.b,vec2: Math.mulVecScalar(deltaB,scalar: lr))
			if(self.settings["log level"] > 0) {
				let iT = T() is Double ? i.toDouble() as! T : i as! T
				let hu = T() is Double ? Double(100) as! T : 100 as! T
				let progress = (Math.one() * iT / epochs) * hu
				if(progress > currentProgress) {
					print("LogisticRegression: \(progress.toInt())% Completed.")
					currentProgress += Math.one()
				}
			}
		}
		if(self.settings["log level"] > 0) {
			print("LogisticRegression Final Cross Entropy : \(self.getReconstructionCrossEntropy())")
		}
	}

	public func getReconstructionCrossEntropy() -> T {
		let probYgivenX = Math.softmaxMat(Math.addMatVec(Math.mulMat(self.x,mat2: self.W),vec: self.b))
		let a = Math.mulMatElementWise(self.y, mat2: Math.activateMat(probYgivenX){ x in
			if T() is Double {
				return log(x.toDouble()) as! T
			}
				return log(x.toDouble()).toInt() as! T
			})
		let mat2: [[T]] = Math.activateMat(Math.mulMatScalar(Math.addMatScalar(probYgivenX,scalar: -Math.one()),scalar: -Math.one())){ x in
			if T() is Double {
				return log(x.toDouble()) as! T
			}
			return log(x.toDouble()).toInt() as! T
			}
		let b = Math.mulMatElementWise(Math.mulMatScalar(Math.addMatScalar(self.y,scalar: -Math.one()),scalar: -Math.one()),
		                               mat2: mat2)
		let crossEntropy = -Math.meanVec(Math.sumMatAxis(Math.addMat(a,mat2: b),axis: 1))
		return crossEntropy
	}

	public func predict(x: [[T]]) -> [[T]] {
		return Math.softmaxMat(Math.addMatVec(Math.mulMat(x,mat2: self.W),vec: self.b))
	}

	public func set(property: String, value: Int) {
		self.settings[property] = value
	}
}


var x: [[Double]] = [[1,1,1,0,0,0],
         [1,0,1,0,0,0],
         [1,1,1,0,0,0],
         [0,0,1,1,1,0],
         [0,0,1,1,0,0],
         [0,0,1,1,1,0]]
var y: [[Double]] = [[1, 0],
         [1, 0],
         [1, 0],
         [0, 1],
         [0, 1],
         [0, 1]]

var classifier = LogisticRegression<Double>(settings: [
	"input" : x,
	"label" : y,
	"n_in" : 6,
	"n_out" : 2
])

classifier.set("log level",value: 1)

var training_epochs = 800.0, lr = 0.01

classifier.train([
	"lr" : lr,
	"epochs" : training_epochs
])

x = [[1, 1, 0, 0, 0, 0],
     [0, 0, 0, 1, 1, 0],
     [1, 1, 1, 1, 1, 0]];

print("Result : \(classifier.predict(x))")

/*:
****
[Next](@next)
*/